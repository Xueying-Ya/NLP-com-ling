{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GEC_KenLM2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c9fc5544d6f741c386d4ec6cf2af502f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a26921ed059c43bca7b967ddee9c0d57",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bafcdab67b74413ca02f5b43b80193ae",
              "IPY_MODEL_7421501a436b458294eb6219d0f23b04"
            ]
          }
        },
        "a26921ed059c43bca7b967ddee9c0d57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bafcdab67b74413ca02f5b43b80193ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2e627d7e1b2b4090bf8a3ff05970061d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 57151,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 57151,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_966328cc2f334169b9875d8afbbca4a6"
          }
        },
        "7421501a436b458294eb6219d0f23b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_01e0c001ec8e4d63b160c0ee54039dea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 57151/57151 [20:07&lt;00:00, 47.31it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15fa00ba246240479c7b4e1d1933ede5"
          }
        },
        "2e627d7e1b2b4090bf8a3ff05970061d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "966328cc2f334169b9875d8afbbca4a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01e0c001ec8e4d63b160c0ee54039dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15fa00ba246240479c7b4e1d1933ede5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98efb6bd7d3f453e81178f9210fec0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7ded8ee5944f4168bfb7e729b561cafb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92bba3f0386e44d39a66b17180b807b7",
              "IPY_MODEL_b21c416aaa274781bb6fa0c4723ebac6"
            ]
          }
        },
        "7ded8ee5944f4168bfb7e729b561cafb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92bba3f0386e44d39a66b17180b807b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d51acae53e3406f8fd2280595d5e36e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 57151,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 57151,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48f87a487da54a9db5066541f1d8909d"
          }
        },
        "b21c416aaa274781bb6fa0c4723ebac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4c6b1539ea3b42f4bf4ac755d75d3a79",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 57151/57151 [11:47&lt;00:00, 80.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f2253fad091444ab3fc374855fa9f12"
          }
        },
        "9d51acae53e3406f8fd2280595d5e36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48f87a487da54a9db5066541f1d8909d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c6b1539ea3b42f4bf4ac755d75d3a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f2253fad091444ab3fc374855fa9f12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a16ec8e6f2b47418fdf109d757089eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_462bf8f9d2fc489c869ea614fed06f03",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d640635e442b4dc2a7d2bf5e767a8558",
              "IPY_MODEL_75d125ea319348058ba9d1e1ea08f4ff"
            ]
          }
        },
        "462bf8f9d2fc489c869ea614fed06f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d640635e442b4dc2a7d2bf5e767a8558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6cc066f5889a40e593b5d571de75b19e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 57151,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 57151,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0da5187fa5be4e3684bec40853962617"
          }
        },
        "75d125ea319348058ba9d1e1ea08f4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0f790026c0348008be2f26aba5be00f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 57151/57151 [51:06&lt;00:00, 18.64it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2dc2d4a15ab74b04a88d1b76607b4a17"
          }
        },
        "6cc066f5889a40e593b5d571de75b19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0da5187fa5be4e3684bec40853962617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0f790026c0348008be2f26aba5be00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2dc2d4a15ab74b04a88d1b76607b4a17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMaxpmiLKk2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "96af028b-da49-4a0a-9220-d32c4fe74fa1"
      },
      "source": [
        "#test set\n",
        "!gdown --id 1CHnRNybDYbq9xTZNCxf22PIQIyBe0Yfz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CHnRNybDYbq9xTZNCxf22PIQIyBe0Yfz\n",
            "To: /content/gec-test-set.txt\n",
            "\r  0% 0.00/162k [00:00<?, ?B/s]\r100% 162k/162k [00:00<00:00, 23.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1MzOIXKj1Te",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2aeeacd-a325-497f-b6e1-6439c4c56990"
      },
      "source": [
        "!pip3 install pyinflect\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyinflect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/ca/123642f8be91a61cb7121c6dd9794a2ddedc36c381ae0a44fd3f5cd041b1/pyinflect-0.5.1-py3-none-any.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyinflect\n",
            "Successfully installed pyinflect-0.5.1\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr1VvJzHoM6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer() "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxgk_5O6oTCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pos_tag(sentence):\n",
        "   tokens = word_tokenize(sentence)\n",
        "   return nltk.pos_tag(tokens)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_1wlHhJY_XG",
        "colab_type": "text"
      },
      "source": [
        "# **Function for generating candidate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tumkPpImjpjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyinflect import getAllInflections, getInflection\n",
        "from nltk import word_tokenize\n",
        "import spacy\n",
        "import re\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDsm_UXcOAcd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "73513f29-bb99-4c19-c466-9df523a1927c"
      },
      "source": [
        "!git clone https://github.com/nozomiyamada/contest2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'contest2'...\n",
            "remote: Enumerating objects: 169, done.\u001b[K\n",
            "remote: Counting objects: 100% (169/169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (152/152), done.\u001b[K\n",
            "remote: Total 169 (delta 47), reused 94 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (169/169), 6.59 MiB | 12.48 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVB14_3aOy2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "#noun.csv\n",
        "noun_dict2 = dict()\n",
        "noun_list = []\n",
        "with open('/content/contest2/noun.json') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  for row in csv_reader:\n",
        "    for string in row:\n",
        "      noun_list.append(string.strip(' ').strip(\"]\").strip(\"[\").strip(' \" '))\n",
        "for i in range(len(noun_list)-1):\n",
        "  noun_dict2[noun_list[i]] = noun_list[i : i+2]\n",
        "\n",
        "#verb.csv\n",
        "verb_dict2 = dict()\n",
        "with open('/content/contest2/verbs-dictionaries.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file,delimiter =',')\n",
        "  for row in csv_reader:\n",
        "    for verbs in row:\n",
        "      verbs_list = verbs.split('\\t')\n",
        "      verb_dict2[verbs_list[0]] = verbs_list[:]\n",
        "\n",
        "#prep.json\n",
        "prep_list = []\n",
        "with open('/content/contest2/prep.json') as f:\n",
        "  data = json.load(f)\n",
        "  #prep_list.append(\"\")\n",
        "  for prep in data:\n",
        "    prep_list.append(prep)\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5yBM1RjuwEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modal = [\"will\",\"would\",\"must\",\"shall\",\"should\",\"may\", \"might\",\"can\",\"could\" ]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYVqzxlmIReq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#find candidates\n",
        "\n",
        "def pos_tag(sentence):\n",
        "   tokens = word_tokenize(sentence)\n",
        "   return nltk.pos_tag(tokens)\n",
        "\n",
        "def lemma(word):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(word)\n",
        "  lemma_word = [token.lemma_ for token in doc]\n",
        "  return lemma_word\n",
        "\n",
        "def verb_form_candidate(word):\n",
        "  infection_dict = getAllInflections(word , 'V')\n",
        "  if word in verb_dict2:\n",
        "    get_infection = verb_dict2[word]\n",
        "  else:\n",
        "    get_infection = [tup[0] for tup in infection_dict.values()]\n",
        "    get_infection.append(word)\n",
        "  return get_infection\n",
        "\n",
        "def be_verb():\n",
        "  return ['be', 'is', 'am', 'are','was', 'were', 'been','being']\n",
        "\n",
        "def article_candidate():\n",
        "  return ['an','a','the',\"some\",'any','this','that','those','these', '']#,\"each\",'every','all','either','neither']\n",
        "\n",
        "def  noun_candidate(word):\n",
        "  noun_dict1 = getAllInflections(word , 'N')\n",
        "  if len(word) > 1 and word in noun_dict2:\n",
        "    get_forms = noun_dict2[word]\n",
        "  else:\n",
        "    get_forms = [tup[0] for tup in noun_dict1.values()]\n",
        "    if len(get_forms) == 0:\n",
        "      get_forms.append(word)\n",
        "      get_forms.append(word + 's')\n",
        "      if word.endswith('y') and len(word) > 1 and word[-2] not in ['a','e','i','o','u']:\n",
        "        get_forms.append(word[ : -1] + 'ies')\n",
        "  return get_forms\n",
        "\n",
        "def prep_candidate():\n",
        "   return prep_list\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdlHjc3Lx7Ep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cfcd0f3d-6360-44ac-dcdc-5cd1b0bcf54c"
      },
      "source": [
        "prep_candidate()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aboard',\n",
              " 'about',\n",
              " 'above',\n",
              " 'absent',\n",
              " 'across',\n",
              " 'after',\n",
              " 'against',\n",
              " 'along',\n",
              " 'alongside',\n",
              " 'amid',\n",
              " 'amidst',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'around',\n",
              " 'as',\n",
              " 'astride',\n",
              " 'at',\n",
              " 'atop',\n",
              " 'before',\n",
              " 'afore',\n",
              " 'behind',\n",
              " 'below',\n",
              " 'beneath',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'by',\n",
              " 'circa',\n",
              " 'despite',\n",
              " 'down',\n",
              " 'during',\n",
              " 'except',\n",
              " 'for',\n",
              " 'from',\n",
              " 'in',\n",
              " 'inside',\n",
              " 'into',\n",
              " 'less',\n",
              " 'like',\n",
              " 'minus',\n",
              " 'near',\n",
              " 'nearer',\n",
              " 'nearest',\n",
              " 'notwithstanding',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'onto',\n",
              " 'opposite',\n",
              " 'outside',\n",
              " 'over',\n",
              " 'past',\n",
              " 'per',\n",
              " 'save',\n",
              " 'since',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'to',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'under',\n",
              " 'underneath',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'upside',\n",
              " 'versus',\n",
              " 'via',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'worth',\n",
              " 'according to',\n",
              " 'adjacent to',\n",
              " 'ahead of',\n",
              " 'apart from',\n",
              " 'as of',\n",
              " 'as per',\n",
              " 'as regards',\n",
              " 'aside from',\n",
              " 'astern of',\n",
              " 'back to',\n",
              " 'because of',\n",
              " 'close to',\n",
              " 'due to',\n",
              " 'except for',\n",
              " 'far from',\n",
              " 'inside of',\n",
              " 'instead of',\n",
              " 'left of',\n",
              " 'near to',\n",
              " 'next to',\n",
              " 'opposite of',\n",
              " 'opposite to',\n",
              " 'out from',\n",
              " 'out of',\n",
              " 'outside of',\n",
              " 'owing to',\n",
              " 'prior to',\n",
              " 'pursuant to',\n",
              " 'rather than',\n",
              " 'regardless of',\n",
              " 'right of',\n",
              " 'subsequent to',\n",
              " 'such as',\n",
              " 'thanks to',\n",
              " 'up to',\n",
              " 'as far as',\n",
              " 'as opposed to',\n",
              " 'as soon as',\n",
              " 'as well as',\n",
              " 'at the behest of',\n",
              " 'by means of',\n",
              " 'by virtue of',\n",
              " 'for the sake of',\n",
              " 'in accordance with',\n",
              " 'in addition to',\n",
              " 'in case of',\n",
              " 'in front of',\n",
              " 'in lieu of',\n",
              " 'in place of',\n",
              " 'in point of',\n",
              " 'in spite of',\n",
              " 'on account of',\n",
              " 'on behalf of',\n",
              " 'on top of',\n",
              " 'with regard to',\n",
              " 'with respect to',\n",
              " 'with a view to']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfrwk1hQ7oVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff9b7182-7c7e-48f5-f27f-de2b77370f9e"
      },
      "source": [
        "noun_candidate(\"party\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['party', 'parties']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr2T2e_Q3tsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95d6fe71-2c8b-4766-c4ee-91f7ac832143"
      },
      "source": [
        "prep_candidate()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aboard',\n",
              " 'about',\n",
              " 'above',\n",
              " 'absent',\n",
              " 'across',\n",
              " 'after',\n",
              " 'against',\n",
              " 'along',\n",
              " 'alongside',\n",
              " 'amid',\n",
              " 'amidst',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'around',\n",
              " 'as',\n",
              " 'astride',\n",
              " 'at',\n",
              " 'atop',\n",
              " 'before',\n",
              " 'afore',\n",
              " 'behind',\n",
              " 'below',\n",
              " 'beneath',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'by',\n",
              " 'circa',\n",
              " 'despite',\n",
              " 'down',\n",
              " 'during',\n",
              " 'except',\n",
              " 'for',\n",
              " 'from',\n",
              " 'in',\n",
              " 'inside',\n",
              " 'into',\n",
              " 'less',\n",
              " 'like',\n",
              " 'minus',\n",
              " 'near',\n",
              " 'nearer',\n",
              " 'nearest',\n",
              " 'notwithstanding',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'onto',\n",
              " 'opposite',\n",
              " 'outside',\n",
              " 'over',\n",
              " 'past',\n",
              " 'per',\n",
              " 'save',\n",
              " 'since',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'to',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'under',\n",
              " 'underneath',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'upside',\n",
              " 'versus',\n",
              " 'via',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'worth',\n",
              " 'according to',\n",
              " 'adjacent to',\n",
              " 'ahead of',\n",
              " 'apart from',\n",
              " 'as of',\n",
              " 'as per',\n",
              " 'as regards',\n",
              " 'aside from',\n",
              " 'astern of',\n",
              " 'back to',\n",
              " 'because of',\n",
              " 'close to',\n",
              " 'due to',\n",
              " 'except for',\n",
              " 'far from',\n",
              " 'inside of',\n",
              " 'instead of',\n",
              " 'left of',\n",
              " 'near to',\n",
              " 'next to',\n",
              " 'opposite of',\n",
              " 'opposite to',\n",
              " 'out from',\n",
              " 'out of',\n",
              " 'outside of',\n",
              " 'owing to',\n",
              " 'prior to',\n",
              " 'pursuant to',\n",
              " 'rather than',\n",
              " 'regardless of',\n",
              " 'right of',\n",
              " 'subsequent to',\n",
              " 'such as',\n",
              " 'thanks to',\n",
              " 'up to',\n",
              " 'as far as',\n",
              " 'as opposed to',\n",
              " 'as soon as',\n",
              " 'as well as',\n",
              " 'at the behest of',\n",
              " 'by means of',\n",
              " 'by virtue of',\n",
              " 'for the sake of',\n",
              " 'in accordance with',\n",
              " 'in addition to',\n",
              " 'in case of',\n",
              " 'in front of',\n",
              " 'in lieu of',\n",
              " 'in place of',\n",
              " 'in point of',\n",
              " 'in spite of',\n",
              " 'on account of',\n",
              " 'on behalf of',\n",
              " 'on top of',\n",
              " 'with regard to',\n",
              " 'with respect to',\n",
              " 'with a view to']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcbEblnoaKsu",
        "colab_type": "text"
      },
      "source": [
        "# **Sentences**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh5YqX92BcW2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64149ff8-142a-4aa0-e4e8-fd902c2b8eb4"
      },
      "source": [
        "pos_tag('I am a good boy')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('good', 'JJ'), ('boy', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isU67DdXrPdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9de49d2-cf31-4b8f-914a-75636fda8360"
      },
      "source": [
        "pos_tag('I go shopping')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('go', 'VBP'), ('shopping', 'VBG')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBPRXZuadqQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ca815a4-834b-4961-f1a5-d1d505f36abb"
      },
      "source": [
        "with open('/content/contest2/bea19-sentences.txt') as file:\n",
        "  data = file.read().split('\\n')\n",
        "  sentences = [s for s in data]\n",
        "sentences[100]"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"According Khandani 's article , engineers can test the solution by prototyping or concurrent engineering .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g0nnQqvaUp1",
        "colab_type": "text"
      },
      "source": [
        "# **LM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdmUvZXaIr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a6cc0c04-5358-44a5-f5bd-f42cf6f8ee65"
      },
      "source": [
        "#Install KenLM Library\n",
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "!mkdir kenlm/build"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-18 09:25:17--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
            "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 490441 (479K) [application/x-gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "\r-                     0%[                    ]       0  --.-KB/s               \r-                   100%[===================>] 478.95K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-08-18 09:25:17 (26.9 MB/s) - written to stdout [490441/490441]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drq07Q5yhtYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef864f34-1c48-4d5d-9c03-35390f912e52"
      },
      "source": [
        "%cd kenlm/build\n",
        "!cmake ..\n",
        "!make -j2"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/kenlm/build\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   program_options\n",
            "--   system\n",
            "--   thread\n",
            "--   unit_test_framework\n",
            "--   chrono\n",
            "--   date_time\n",
            "--   atomic\n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/include (found version \"5.2.2\") \n",
            "-- Could NOT find Eigen3 (missing: EIGEN3_INCLUDE_DIR EIGEN3_VERSION_OK) (Required is at least version \"2.91.0\")\n",
            "CMake Warning at lm/interpolate/CMakeLists.txt:65 (message):\n",
            "  Not building interpolation.  Eigen3 was not found.\n",
            "\n",
            "\n",
            "-- To install Eigen3 in your home directory, copy paste this:\n",
            "export EIGEN3_ROOT=$HOME/eigen-eigen-07105f7124f9\n",
            "(cd $HOME; wget -O - https://bitbucket.org/eigen/eigen/get/3.2.8.tar.bz2 |tar xj)\n",
            "rm CMakeCache.txt\n",
            "\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_filter\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_util\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 16%] Built target kenlm_filter\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 43%] Built target kenlm_util\n",
            "\u001b[35m\u001b[1mScanning dependencies of target probing_hash_table_benchmark\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 55%] Built target probing_hash_table_benchmark\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fragment\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target build_binary\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 76%] Built target fragment\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_benchmark\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 77%] Built target build_binary\n",
            "\u001b[35m\u001b[1mScanning dependencies of target query\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 80%] Built target query\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_builder\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 87%] Built target kenlm_benchmark\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target phrase_table_vocab\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 91%] Built target phrase_table_vocab\n",
            "\u001b[35m\u001b[1mScanning dependencies of target filter\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 93%] Built target kenlm_builder\n",
            "\u001b[35m\u001b[1mScanning dependencies of target count_ngrams\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[ 96%] Built target count_ngrams\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lmplz\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 98%] Built target filter\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[100%] Built target lmplz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFqfETNxh6lf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "c316751f-db05-4766-9e54-8a64d13c60d9"
      },
      "source": [
        "#Dowload news data \n",
        "!gdown --id 1tGt5FJIZ0RPKAz4NxxhL1PvNd85AJBQu"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tGt5FJIZ0RPKAz4NxxhL1PvNd85AJBQu\n",
            "To: /content/kenlm/build/gigaword3_nyt_eng.tar.gz\n",
            "2.54GB [00:31, 80.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfcQBBm1h-Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf gigaword3_nyt_eng.tar.gz"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdLu_0pxG6ly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6f1e24d0-741e-4c2a-d38e-301e32a0a208"
      },
      "source": [
        "#Dowload gutenberg\n",
        "!gdown --id 1PeMJ8Z9HBDravgGzZ1baDRnliHRf1O75"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PeMJ8Z9HBDravgGzZ1baDRnliHRf1O75\n",
            "To: /content/kenlm/build/Gutenberg.zip\n",
            "462MB [00:06, 67.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec6OIPGPHCjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('/content/kenlm/build/Gutenberg.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciFz8HRFiCpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "e3034b9f-c750-4292-d4eb-1d40cb2d90d0"
      },
      "source": [
        "ls #/content/kenlm/build/kenlm/build/kenlm"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbin\u001b[0m/                      nyt_eng_199701  nyt_eng_200005  nyt_eng_200309\n",
            "CMakeCache.txt            nyt_eng_199702  nyt_eng_200006  nyt_eng_200310\n",
            "\u001b[01;34mCMakeFiles\u001b[0m/               nyt_eng_199703  nyt_eng_200007  nyt_eng_200311\n",
            "cmake_install.cmake       nyt_eng_199704  nyt_eng_200008  nyt_eng_200312\n",
            "gigaword3_nyt_eng.tar.gz  nyt_eng_199705  nyt_eng_200009  nyt_eng_200401\n",
            "\u001b[01;34mGutenberg\u001b[0m/                nyt_eng_199706  nyt_eng_200010  nyt_eng_200402\n",
            "Gutenberg.zip             nyt_eng_199707  nyt_eng_200011  nyt_eng_200403\n",
            "\u001b[01;34mlib\u001b[0m/                      nyt_eng_199708  nyt_eng_200012  nyt_eng_200404\n",
            "\u001b[01;34mlm\u001b[0m/                       nyt_eng_199709  nyt_eng_200101  nyt_eng_200405\n",
            "Makefile                  nyt_eng_199710  nyt_eng_200102  nyt_eng_200407\n",
            "nyt_eng_199407            nyt_eng_199711  nyt_eng_200103  nyt_eng_200408\n",
            "nyt_eng_199408            nyt_eng_199712  nyt_eng_200104  nyt_eng_200409\n",
            "nyt_eng_199409            nyt_eng_199801  nyt_eng_200105  nyt_eng_200410\n",
            "nyt_eng_199410            nyt_eng_199802  nyt_eng_200106  nyt_eng_200411\n",
            "nyt_eng_199411            nyt_eng_199803  nyt_eng_200107  nyt_eng_200412\n",
            "nyt_eng_199412            nyt_eng_199804  nyt_eng_200108  nyt_eng_200501\n",
            "nyt_eng_199501            nyt_eng_199805  nyt_eng_200109  nyt_eng_200502\n",
            "nyt_eng_199502            nyt_eng_199806  nyt_eng_200110  nyt_eng_200503\n",
            "nyt_eng_199503            nyt_eng_199807  nyt_eng_200111  nyt_eng_200504\n",
            "nyt_eng_199504            nyt_eng_199808  nyt_eng_200112  nyt_eng_200505\n",
            "nyt_eng_199505            nyt_eng_199809  nyt_eng_200201  nyt_eng_200506\n",
            "nyt_eng_199506            nyt_eng_199810  nyt_eng_200202  nyt_eng_200507\n",
            "nyt_eng_199507            nyt_eng_199811  nyt_eng_200203  nyt_eng_200508\n",
            "nyt_eng_199508            nyt_eng_199812  nyt_eng_200204  nyt_eng_200509\n",
            "nyt_eng_199509            nyt_eng_199901  nyt_eng_200205  nyt_eng_200510\n",
            "nyt_eng_199510            nyt_eng_199902  nyt_eng_200206  nyt_eng_200511\n",
            "nyt_eng_199511            nyt_eng_199903  nyt_eng_200207  nyt_eng_200512\n",
            "nyt_eng_199512            nyt_eng_199904  nyt_eng_200208  nyt_eng_200601\n",
            "nyt_eng_199601            nyt_eng_199905  nyt_eng_200209  nyt_eng_200602\n",
            "nyt_eng_199602            nyt_eng_199906  nyt_eng_200210  nyt_eng_200603\n",
            "nyt_eng_199603            nyt_eng_199907  nyt_eng_200211  nyt_eng_200604\n",
            "nyt_eng_199604            nyt_eng_199908  nyt_eng_200212  nyt_eng_200605\n",
            "nyt_eng_199605            nyt_eng_199909  nyt_eng_200301  nyt_eng_200606\n",
            "nyt_eng_199606            nyt_eng_199910  nyt_eng_200302  nyt_eng_200607\n",
            "nyt_eng_199607            nyt_eng_199911  nyt_eng_200303  nyt_eng_200608\n",
            "nyt_eng_199608            nyt_eng_199912  nyt_eng_200304  nyt_eng_200609\n",
            "nyt_eng_199609            nyt_eng_200001  nyt_eng_200305  nyt_eng_200610\n",
            "nyt_eng_199610            nyt_eng_200002  nyt_eng_200306  nyt_eng_200611\n",
            "nyt_eng_199611            nyt_eng_200003  nyt_eng_200307  nyt_eng_200612\n",
            "nyt_eng_199612            nyt_eng_200004  nyt_eng_200308  \u001b[01;34mutil\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrRXTngNWLmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat nyt_eng_2001* nyt_eng_2006*  > nyt_eng.txt"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2V7HKfXiIBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "03422651-c33c-4b7f-c421-5f3bfeef4c78"
      },
      "source": [
        "#Install KenLM in Python\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[K     - 2.8MB 4.0MB/s\n",
            "\u001b[?25hBuilding wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp36-cp36m-linux_x86_64.whl size=2330440 sha256=90df06d66938ea75ba776e64312a16e5f7cc7c284b859e05b180fa5ceb28209d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2jyifpcb/wheels/2d/32/73/e3093c9d11dc8abf79c156a4db1a1c5631428059d4f9ff2cba\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ErtpzmiPc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "66b95f0f-a6bf-4442-fa2c-bd91195441a6"
      },
      "source": [
        "#Train LM with KenLM\n",
        "!bin/lmplz -o 6 --text nyt_eng.txt --arpa sixgram.arpa"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/kenlm/build/nyt_eng.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 3456286720 bytes == 0x55bd2a102000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd2846d358 0x55bd2844c290 0x55bd28438096 0x7fbb437eab97 0x55bd28439ada\n",
            "tcmalloc: large alloc 18433531904 bytes == 0x55bdf812e000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd284c37aa 0x55bd284c41c8 0x55bd2844c2ad 0x55bd28438096 0x7fbb437eab97 0x55bd28439ada\n",
            "****************************************************************************************************\n",
            "Unigram tokens 209075440 types 880985\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:10571820 2:1347963776 3:2527431936 4:4043891200 5:5897341440 6:8087782400\n",
            "tcmalloc: large alloc 8087789568 bytes == 0x55bd2a102000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd284c37aa 0x55bd284c41c8 0x55bd2844c84e 0x55bd28438096 0x7fbb437eab97 0x55bd28439ada\n",
            "tcmalloc: large alloc 2527436800 bytes == 0x55bf5d1c4000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd284c37aa 0x55bd284c41c8 0x55bd2844cc3d 0x55bd28438096 0x7fbb437eab97 0x55bd28439ada\n",
            "tcmalloc: large alloc 4043898880 bytes == 0x55bff3c1e000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd284c37aa 0x55bd284c41c8 0x55bd2844cc3d 0x55bd28438096 0x7fbb437eab97 0x55bd28439ada\n",
            "tcmalloc: large alloc 5897347072 bytes == 0x55c243d68000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd284c37aa 0x55bd284c41c8 0x55bd2844cc3d 0x55bd28438096 0x7fbb437eab97 0x55bd28439ada\n",
            "Statistics:\n",
            "1 880985 D1=0.680597 D2=0.997432 D3+=1.31674\n",
            "2 15721743 D1=0.722756 D2=1.08151 D3+=1.38691\n",
            "3 60179756 D1=0.80287 D2=1.14142 D3+=1.35163\n",
            "4 104114180 D1=0.874009 D2=1.24055 D3+=1.33935\n",
            "5 120994436 D1=0.924204 D2=1.37909 D3+=1.28531\n",
            "6 118342352 D1=0.806938 D2=1.6065 D3+=0.878681\n",
            "Memory estimate for binary LM:\n",
            "type       MB\n",
            "probing  8942 assuming -p 1.5\n",
            "probing 10668 assuming -r models -p 1.5\n",
            "trie     4684 without quantization\n",
            "trie     2673 assuming -q 8 -b 8 quantization \n",
            "trie     3979 assuming -a 22 array pointer compression\n",
            "trie     1968 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "tcmalloc: large alloc 2498748416 bytes == 0x55c24bd68000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd28457232 0x55bd284581cd 0x7fbb44f84bcd 0x7fbb44d5b6db 0x7fbb438eaa3f\n",
            "tcmalloc: large alloc 3387850752 bytes == 0x55c24bd68000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd28457232 0x55bd284581cd 0x7fbb44f84bcd 0x7fbb44d5b6db 0x7fbb438eaa3f\n",
            "tcmalloc: large alloc 3786956800 bytes == 0x55c24bd68000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd28457232 0x55bd284581cd 0x7fbb44f84bcd 0x7fbb44d5b6db 0x7fbb438eaa3f\n",
            "Chain sizes: 1:10571820 2:251547888 3:1203595120 4:2498740320 5:3387844208 6:3786955264\n",
            "tcmalloc: large alloc 3387850752 bytes == 0x55bd2a102000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd284c37aa 0x55bd284c41c8 0x55bd28452cb1 0x55bd2844ac2e 0x55bd2844d2e9 0x55bd28438096 0x7fbb437eab97 0x55bd28439ada\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 3786956800 bytes == 0x55bdf3fea000 @  0x7fbb456511e7 0x55bd284d9772 0x55bd284c37aa 0x55bd284c41c8 0x55bd28452cb1 0x55bd2844ac2e 0x55bd2844d2e9 0x55bd28438096 0x7fbb437eab97 0x55bd28439ada\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:10571820 2:251547888 3:1203595120 4:2498740320 5:3387844208 6:3786955264\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:27684460 kB\tVmRSS:10915076 kB\tRSSMax:15104628 kB\tuser:595.999\tsys:114.822\tCPU:710.821\treal:1490.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w3w1PAGigRP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "488d40e0-2eaf-4908-b39c-103b9662f4da"
      },
      "source": [
        "ls"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbin\u001b[0m/                      nyt_eng_199702  nyt_eng_200007  nyt_eng_200312\n",
            "CMakeCache.txt            nyt_eng_199703  nyt_eng_200008  nyt_eng_200401\n",
            "\u001b[01;34mCMakeFiles\u001b[0m/               nyt_eng_199704  nyt_eng_200009  nyt_eng_200402\n",
            "cmake_install.cmake       nyt_eng_199705  nyt_eng_200010  nyt_eng_200403\n",
            "gigaword3_nyt_eng.tar.gz  nyt_eng_199706  nyt_eng_200011  nyt_eng_200404\n",
            "\u001b[01;34mGutenberg\u001b[0m/                nyt_eng_199707  nyt_eng_200012  nyt_eng_200405\n",
            "Gutenberg.zip             nyt_eng_199708  nyt_eng_200101  nyt_eng_200407\n",
            "\u001b[01;34mlib\u001b[0m/                      nyt_eng_199709  nyt_eng_200102  nyt_eng_200408\n",
            "\u001b[01;34mlm\u001b[0m/                       nyt_eng_199710  nyt_eng_200103  nyt_eng_200409\n",
            "Makefile                  nyt_eng_199711  nyt_eng_200104  nyt_eng_200410\n",
            "nyt_eng_199407            nyt_eng_199712  nyt_eng_200105  nyt_eng_200411\n",
            "nyt_eng_199408            nyt_eng_199801  nyt_eng_200106  nyt_eng_200412\n",
            "nyt_eng_199409            nyt_eng_199802  nyt_eng_200107  nyt_eng_200501\n",
            "nyt_eng_199410            nyt_eng_199803  nyt_eng_200108  nyt_eng_200502\n",
            "nyt_eng_199411            nyt_eng_199804  nyt_eng_200109  nyt_eng_200503\n",
            "nyt_eng_199412            nyt_eng_199805  nyt_eng_200110  nyt_eng_200504\n",
            "nyt_eng_199501            nyt_eng_199806  nyt_eng_200111  nyt_eng_200505\n",
            "nyt_eng_199502            nyt_eng_199807  nyt_eng_200112  nyt_eng_200506\n",
            "nyt_eng_199503            nyt_eng_199808  nyt_eng_200201  nyt_eng_200507\n",
            "nyt_eng_199504            nyt_eng_199809  nyt_eng_200202  nyt_eng_200508\n",
            "nyt_eng_199505            nyt_eng_199810  nyt_eng_200203  nyt_eng_200509\n",
            "nyt_eng_199506            nyt_eng_199811  nyt_eng_200204  nyt_eng_200510\n",
            "nyt_eng_199507            nyt_eng_199812  nyt_eng_200205  nyt_eng_200511\n",
            "nyt_eng_199508            nyt_eng_199901  nyt_eng_200206  nyt_eng_200512\n",
            "nyt_eng_199509            nyt_eng_199902  nyt_eng_200207  nyt_eng_200601\n",
            "nyt_eng_199510            nyt_eng_199903  nyt_eng_200208  nyt_eng_200602\n",
            "nyt_eng_199511            nyt_eng_199904  nyt_eng_200209  nyt_eng_200603\n",
            "nyt_eng_199512            nyt_eng_199905  nyt_eng_200210  nyt_eng_200604\n",
            "nyt_eng_199601            nyt_eng_199906  nyt_eng_200211  nyt_eng_200605\n",
            "nyt_eng_199602            nyt_eng_199907  nyt_eng_200212  nyt_eng_200606\n",
            "nyt_eng_199603            nyt_eng_199908  nyt_eng_200301  nyt_eng_200607\n",
            "nyt_eng_199604            nyt_eng_199909  nyt_eng_200302  nyt_eng_200608\n",
            "nyt_eng_199605            nyt_eng_199910  nyt_eng_200303  nyt_eng_200609\n",
            "nyt_eng_199606            nyt_eng_199911  nyt_eng_200304  nyt_eng_200610\n",
            "nyt_eng_199607            nyt_eng_199912  nyt_eng_200305  nyt_eng_200611\n",
            "nyt_eng_199608            nyt_eng_200001  nyt_eng_200306  nyt_eng_200612\n",
            "nyt_eng_199609            nyt_eng_200002  nyt_eng_200307  nyt_eng.txt\n",
            "nyt_eng_199610            nyt_eng_200003  nyt_eng_200308  sixgram.arpa\n",
            "nyt_eng_199611            nyt_eng_200004  nyt_eng_200309  \u001b[01;34mutil\u001b[0m/\n",
            "nyt_eng_199612            nyt_eng_200005  nyt_eng_200310\n",
            "nyt_eng_199701            nyt_eng_200006  nyt_eng_200311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUgYZBwKilOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import kenlm\n",
        "model = kenlm.Model('sixgram.arpa')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8sGvvh9InWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, s):\n",
        "  tokens = s.split(' ')\n",
        "  log_score = 0.0\n",
        "  for i, (logprob, length, oov) in enumerate(model.full_scores(s,bos = True, eos = True)):\n",
        "    if i < len(tokens):\n",
        "     tokens[i], math.exp(logprob), oov\n",
        "    else:\n",
        "      'END', math.exp(logprob), oov\n",
        "  \n",
        "    log_score += logprob\n",
        "  return log_score"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6NuFB80imEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def print_score(model, s):\n",
        "  tokens = s.split(' ')\n",
        "  log_score = 0.0\n",
        "  for i, (logprob, length, oov) in enumerate(model.full_scores(s)):\n",
        "    if i < len(tokens):\n",
        "      print(tokens[i], math.exp(logprob), oov)\n",
        "    else:\n",
        "      print('END', math.exp(logprob), oov)\n",
        "  \n",
        "    log_score += logprob\n",
        "  return log_score"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM5UgAsQiqew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "e21e9b21-d8ed-4c4b-fc13-662e51ef7b13"
      },
      "source": [
        "print_score(model, \"I can't hardly believe what he said.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I 0.07113961029170372 False\n",
            "can't 6.730817696776554e-05 True\n",
            "hardly 0.01296525295734166 False\n",
            "believe 0.05890259809413126 False\n",
            "what 0.24527611272994368 False\n",
            "he 0.38032571191507625 False\n",
            "said. 0.00011563156058634109 False\n",
            "END 0.11392771131013155 False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-33.03608298301697"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfCBlYsAlKlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Edit function\n",
        "\n",
        "def edit_article(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  best_candidate = sentence\n",
        "  for candidate in article_candidate():\n",
        "      new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "      sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0]\n",
        "  return best_candidate\n",
        "\n",
        "def edit_be_verb(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  for candidate in be_verb():\n",
        "    new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "    sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0]\n",
        "  return best_candidate\n",
        "\n",
        "def edit_verb_forms(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  word =lemma(word)[0]\n",
        "  for candidate in verb_form_candidate(word):\n",
        "    new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "    sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0]\n",
        "  return best_candidate\n",
        "\n",
        "def edit_noun(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  word =lemma(word)[0]\n",
        "  for candidate in noun_candidate(word):\n",
        "    new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "    sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0]\n",
        "  return best_candidate\n",
        "\n",
        "def edit_part_of_speech(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  word =lemma(word)[0]\n",
        "  for candidate in part_of_speech_candidate(word):\n",
        "    new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "    sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0]\n",
        "  return best_candidate"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkSA6AqCLFqb",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence Version1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8lAqzbpxZgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edit_sentence1(sentence):\n",
        "    sentence_score = evaluate(model,sentence)\n",
        "    pos_sentence = pos_tag(sentence)\n",
        "    best_candidate = sentence\n",
        "\n",
        "    #article\n",
        "    for word,pos in pos_sentence:\n",
        "        if word in article_candidate():\n",
        "          best_candidate = edit_article(word,sentence)\n",
        "\n",
        "    #be_verb\n",
        "    for word,pos in pos_sentence:\n",
        "      if word in be_verb():\n",
        "        best_candidate = edit_be_verb(word,best_candidate)\n",
        "\n",
        "    #verb forms\n",
        "    for word,pos in pos_sentence:\n",
        "        if pos.startswith('V'):\n",
        "          best_candidate = edit_verb_forms(word,best_candidate)\n",
        "\n",
        "    #noun\n",
        "    for word,pos in pos_sentence:\n",
        "        if pos.startswith('N'):\n",
        "          best_candidate = edit_noun(word,best_candidate)\n",
        "\n",
        "    return best_candidate"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf_4TYTnQDCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32ab4876-7640-427f-97fe-5fb01735392d"
      },
      "source": [
        "edit_sentence1('I play the ball with cat')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I played  ball with cats'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFfttlPieFA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7f3a78a6-5118-4292-9675-72c01250a188"
      },
      "source": [
        "edit_sentence1(sentences[100])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Genetic testing was made possible and available for one 's decision to undergo .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "139LOV4c_MMn",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence Version 2**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoliDC5SS75r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ouput : corrected_file_1\n",
        "#data , tokens : nyt_eng_200001 /  9246188 tokens , 5 grams\n",
        "\n",
        "def edit_sentence2(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   for i in range(len(doc)):\n",
        "    tokens = [token.text for token in nlp(sentence)]\n",
        "    if doc[i].pos_ == \"NOUN\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in noun_candidate(doc[i].lemma_):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif doc[i].text in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif doc[i].text in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif doc[i].pos_ == \"VERB\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in verb_form_candidate(doc[i].lemma_):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "    \n",
        "    #Preposition\n",
        "    elif doc[i].pos_ == \"ADP\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "  return sentence.capitalize() + '.'"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dASGhHLh7FDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b41f41e9-9a36-4f1b-a94e-44f9559787cf"
      },
      "source": [
        "verb_dict2[\"be\"]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be', 'is', 'was', 'been', 'being']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnhl6vCCqZEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10755680-ab86-4da1-bfbd-be06de98a2e8"
      },
      "source": [
        "sentences[100]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Genetic testing is made possible and available for one 's decision to undergo .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i1V9282JlNJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "05e7c631-723d-4241-81c4-f6b822706043"
      },
      "source": [
        "edit_sentence2(sentences[100])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Genetic testing was made possible and available for one 's decision to undergo ..\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mniSSB6YzJuw",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence version 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rjt-SfJzJVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ouput : corrected_file_2\n",
        "#data , tokens : Gutenberg , 210907708 tokens\n",
        "\n",
        "def edit_sentence3(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   for i in range(len(doc)):\n",
        "    tokens = [token.text for token in nlp(sentence)]\n",
        "    if doc[i].pos_ == \"NOUN\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in noun_candidate(doc[i].lemma_):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif doc[i].text in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif doc[i].text in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif doc[i].pos_ == \"VERB\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in verb_form_candidate(doc[i].lemma_):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "      \n",
        "    #Preposition\n",
        "    elif doc[i].pos_ == \"ADP\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "  return sentence.capitalize() + '.'"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvt26ZhD6wJP",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence Version4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xuDKIaZGNyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def join_tokens(tokens):\n",
        "  result = \" \".join(tokens)\n",
        "  if len(result) > 2:\n",
        "    result = result[0].upper() + result[1:]\n",
        "  else:\n",
        "    result = result\n",
        "  return result"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUiNXC4VXshR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edit_sentence4(sentence):\n",
        "  pos_sentence = pos_tag(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   tokens = [token for token,pos in pos_sentence]\n",
        "   for i in range(len(pos_sentence)):\n",
        "    \n",
        "    if pos_sentence[i][1].startswith(\"N\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      for candidate in noun_candidate(stem):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif pos_sentence[i][0] in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif pos_sentence[i][0] in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif pos_sentence[i][1].startswith(\"V\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = nltk.stem.WordNetLemmatizer().lemmatize(pos_sentence[i][0], 'v')\n",
        "\n",
        "      for candidate in verb_form_candidate(stem):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "\n",
        "      if i < len(pos_sentence)-1 and pos_sentence[i+1][0] not in prep_candidate():\n",
        "        sentence_candidates = []\n",
        "        for candidate in prep_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          if candidate == \"\" :\n",
        "             new_tokens[i] = best_candidate\n",
        "          else:\n",
        "            new_tokens[i] = best_candidate + \" \" + candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_tokens[i],evaluate(model,new_sentence)))\n",
        "        rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "        best_candidate = rank[0][0]\n",
        "\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "    elif pos_sentence[i][1].startswith(\"J\"):\n",
        "      sentence_candidates = []\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      if stem in verb_form_candidate(stem):\n",
        "        for vj_can in verb_form_candidate(stem):        \n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = vj_can\n",
        "          sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((vj_can,evaluate(model,sentence)))\n",
        "        rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "        best_candidate = rank[0][0]\n",
        "      else:\n",
        "        best_candidate = pos_sentence[i][0]\n",
        "     \n",
        "    #Preposition\n",
        "    elif pos_sentence[i][0] in prep_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate \n",
        "  return join_tokens(tokens)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NcSJCk88d_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a6b5903-0679-4781-d75d-6e5232f14899"
      },
      "source": [
        "edit_sentence4('I can eat')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I can eat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKoiKC_TAZBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d6d540c9-0515-4035-c610-eb10a06858b0"
      },
      "source": [
        "edit_sentence4(sentences[1114])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'However , it is good for sharing of information .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWDDaPSd0jQb",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence Version5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1akg_Xdn6s4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edit_sentence5(sentence):\n",
        "  pos_sentence = pos_tag(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   tokens = [token for token,pos in pos_sentence]\n",
        "   for i in range(len(pos_sentence)):\n",
        "    \n",
        "    if pos_sentence[i][1].startswith(\"N\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      for candidate in noun_candidate(stem):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "  \n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif pos_sentence[i][0] in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif pos_sentence[i][0] in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "   \n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif pos_sentence[i][1].startswith(\"V\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = nltk.stem.WordNetLemmatizer().lemmatize(pos_sentence[i][0], 'v')\n",
        "\n",
        "      for candidate in verb_form_candidate(stem):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "    elif pos_sentence[i][1].startswith(\"J\"):\n",
        "      sentence_candidates = []\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      if stem in verb_form_candidate(stem):\n",
        "        for vj_can in verb_form_candidate(stem):        \n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = vj_can\n",
        "          sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((vj_can,evaluate(model,sentence)))\n",
        "        rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "        best_candidate = rank[0][0]\n",
        "      else:\n",
        "        best_candidate = pos_sentence[i][0]\n",
        "     \n",
        "    #Preposition\n",
        "    elif pos_sentence[i][0] in prep_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate \n",
        "  return join_tokens(tokens)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrQ881vKS5eu",
        "colab_type": "text"
      },
      "source": [
        "# Edit sentence Version5 with auto correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKIqwUtHJ9Yr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "79f3f44a-8fb8-4416-8ab0-726f59827ee1"
      },
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/5b/6510d8370201fc96cbb773232c2362079389ed3285b0b1c6a297ef6eadc0/autocorrect-2.0.0.tar.gz (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 2.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.0.0-cp36-none-any.whl size=1811641 sha256=7605c629223863243454e874735f897aa85cae2504e48e58540769fa15f23ecd\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/06/bc/e66f28d72bed29591eadc79cebb2e7964ad0282804ab233da3\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE7zRwHRWksT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from autocorrect import Speller\n",
        "\n",
        "spell = Speller(lang='en')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYfTQpSCTHc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edit_sentence5autocorrect(sentence):\n",
        "  sentence = spell(sentence)\n",
        "  pos_sentence = pos_tag(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   tokens = [token for token,pos in pos_sentence]\n",
        "   for i in range(len(pos_sentence)):\n",
        "    \n",
        "    if pos_sentence[i][1].startswith(\"N\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      for candidate in noun_candidate(stem):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif pos_sentence[i][0] in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif pos_sentence[i][0] in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif pos_sentence[i][1].startswith(\"V\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = nltk.stem.WordNetLemmatizer().lemmatize(pos_sentence[i][0], 'v')\n",
        "\n",
        "      for candidate in verb_form_candidate(stem):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "    elif pos_sentence[i][1].startswith(\"J\"):\n",
        "      sentence_candidates = []\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      if stem in verb_form_candidate(stem):\n",
        "        for vj_can in verb_form_candidate(stem):        \n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = vj_can\n",
        "          sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((vj_can,evaluate(model,sentence)))\n",
        "        rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "        best_candidate = rank[0][0]\n",
        "      else:\n",
        "        best_candidate = pos_sentence[i][0]\n",
        "   \n",
        "    #Preposition\n",
        "    elif pos_sentence[i][0] in prep_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate \n",
        "  return join_tokens(tokens)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMIDBqccYv0H",
        "colab_type": "text"
      },
      "source": [
        "#Generate File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1zN1IbMsVm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "c9fc5544d6f741c386d4ec6cf2af502f",
            "a26921ed059c43bca7b967ddee9c0d57",
            "bafcdab67b74413ca02f5b43b80193ae",
            "7421501a436b458294eb6219d0f23b04",
            "2e627d7e1b2b4090bf8a3ff05970061d",
            "966328cc2f334169b9875d8afbbca4a6",
            "01e0c001ec8e4d63b160c0ee54039dea",
            "15fa00ba246240479c7b4e1d1933ede5"
          ]
        },
        "outputId": "1b8fb50c-eb92-4287-ea4a-f53e20b158ff"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "with open('corrected_sentences.txt', 'w') as f :\n",
        "  for line in tqdm_notebook(sentences[:-1]): #ลบบรรทัดสุดท้ายที่เป็นช่องว่าง\n",
        "      f.write(edit_sentence5(line )+ '\\n')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9fc5544d6f741c386d4ec6cf2af502f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=57151.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhZC-iNL1-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2cc4adfe-c16c-49e5-ba0e-93fb41002df4"
      },
      "source": [
        "!wc -l /content/kenlm/build/corrected_sentences.txt"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57151 /content/kenlm/build/corrected_sentences.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9Nqt52rNMmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b701d5a-48dc-4518-f188-92f96589dd77"
      },
      "source": [
        "!wc -l /content/gec-test-set.txt"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1312 /content/gec-test-set.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGrZOKza462k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "98efb6bd7d3f453e81178f9210fec0dc",
            "7ded8ee5944f4168bfb7e729b561cafb",
            "92bba3f0386e44d39a66b17180b807b7",
            "b21c416aaa274781bb6fa0c4723ebac6",
            "9d51acae53e3406f8fd2280595d5e36e",
            "48f87a487da54a9db5066541f1d8909d",
            "4c6b1539ea3b42f4bf4ac755d75d3a79",
            "9f2253fad091444ab3fc374855fa9f12"
          ]
        },
        "outputId": "4bf75908-c623-428e-9900-bb518454798a"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "with open('corrected_file_5_newst.txt', 'w') as f :\n",
        "  for line in tqdm_notebook(sentences[:-1]):\n",
        "    f.write(edit_sentence4(line) + '\\n')"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98efb6bd7d3f453e81178f9210fec0dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=57151.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wAxKS3lTXAU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "3a16ec8e6f2b47418fdf109d757089eb",
            "462bf8f9d2fc489c869ea614fed06f03",
            "d640635e442b4dc2a7d2bf5e767a8558",
            "75d125ea319348058ba9d1e1ea08f4ff",
            "6cc066f5889a40e593b5d571de75b19e",
            "0da5187fa5be4e3684bec40853962617",
            "c0f790026c0348008be2f26aba5be00f",
            "2dc2d4a15ab74b04a88d1b76607b4a17"
          ]
        },
        "outputId": "d822f125-bbc4-4ff1-ca1d-40b6c98a29be"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "with open('corrected_file_5_w_autocorrect.txt', 'w') as f :\n",
        "  for line in tqdm_notebook(sentences[:-1]):\n",
        "    f.write(edit_sentence5autocorrect(line) + '\\n')"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a16ec8e6f2b47418fdf109d757089eb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=57151.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhCT8eS0I_xJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1deb5ff6-4201-4a8a-f4a2-7b7269d28e45"
      },
      "source": [
        "!wc -l /content/kenlm/build/corrected_file_5_newst.txt"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57151 /content/kenlm/build/corrected_file_5_newst.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPJPSKNEJcHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "18937d5b-8d2f-4689-cb4f-61a0cad8237a"
      },
      "source": [
        "!python /content/contest2/m2scorer/scripts/m2scorer.py corrected_file_5_w_autocorrect.txt /content/contest2/nucle.train.gold.bea19.m2"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision   : 0.0636\n",
            "Recall      : 0.1754\n",
            "F_0.5       : 0.0729\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
